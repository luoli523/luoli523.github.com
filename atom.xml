<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[luoli523]]></title>
  <link href="http://luoli523.github.com/atom.xml" rel="self"/>
  <link href="http://luoli523.github.com/"/>
  <updated>2012-11-08T21:11:59+08:00</updated>
  <id>http://luoli523.github.com/</id>
  <author>
    <name><![CDATA[罗李]]></name>
    <email><![CDATA[luoli523@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hadoop作业调优参数整理]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/08/hadoopzuo-ye-diao-you-can-shu-zheng-li/"/>
    <updated>2012-11-08T20:50:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/08/hadoopzuo-ye-diao-you-can-shu-zheng-li</id>
    <content type="html"><![CDATA[<p>新博客开张几天，有好多以前看过我CSDN上博客的同行们在问我以前的笔记还会不会整理到新博客上来，其实原本没有打算搞上来的，因为一来挺耗精力，二来也都比较老了，有一些内容可能大家都已经熟悉，或者有些已经过时了。实在没有想到居然还会有那么多关注以前文章。所以决定，把以前的一些笔记都还是整理到新博客上来。有的可能比较老了，大家请多包含。</p>

<p>今天整理的是这一篇：“hadoop作业调优参数整理”</p>

<!-- more  -->


<h2>Map side tuning参数</h2>

<h3>MapTask运行内部原理</h3>

<p><img src="http://luoli523.github.com/static/mapTask.jpg" alt="mapTaks" /></p>

<p>开始运算，并产生中间数据时，其产生的中间结果并非直接就简单的写入磁盘。这中间的过程比较复杂，并且利用到了内存buffer来进行已经产生的部分结果的缓存，并在内存buffer中进行一些预排序来优化整个map的性能。如上图所示，每一个map都会对应存在一个内存buffer（MapOutputBuffer，即上图的buffer in memory），map会将已经产生的部分结果先写入到该buffer中，这个buffer默认是100MB大小，但是这个大小是可以根据job提交时的参数设定来调整的，该参数即为：io.sort.mb。当map的产生数据非常大时，并且把io.sort.mb调大，那么map在整个计算过程中spill的次数就势必会降低，map task对磁盘的操作就会变少，如果map tasks的瓶颈在磁盘上，这样调整就会大大提高map的计算性能。map做sort和spill的内存结构如下如所示：<br/>
<img src="http://luoli523.github.com/static/mapSpill1.jpg" alt="mapSpill1" /></p>

<p>map在运行过程中，不停的向该buffer中写入已有的计算结果，但是该buffer并不一定能将全部的map输出缓存下来，当map输出超出一定阈值（比如100M），那么map就必须将该buffer中的数据写入到磁盘中去，这个过程在mapreduce中叫做spill。map并不是要等到将该buffer全部写满时才进行spill，因为如果全部写满了再去写spill，势必会造成map的计算部分等待buffer释放空间的情况。所以，map其实是当buffer被写满到一定程度（比如80%）时，就开始进行spill。这个阈值也是由一个job的配置参数来控制，即io.sort.spill.percent，默认为0.80或80%。这个参数同样也是影响spill频繁程度，进而影响map task运行周期对磁盘的读写频率的。但非特殊情况下，通常不需要人为的调整。调整io.sort.mb对用户来说更加方便。</p>

<p>当map task的计算部分全部完成后，如果map有输出，就会生成一个或者多个spill文件，这些文件就是map的输出结果。map在正常退出之前，需要将这些spill合并（merge）成一个，所以map在结束之前还有一个merge的过程。merge的过程中，有一个参数可以调整这个过程的行为，该参数为：io.sort.factor。该参数默认为10。它表示当merge spill文件时，最多能有多少并行的stream向merge文件中写入。比如如果map产生的数据非常的大，产生的spill文件大于10，而io.sort.factor使用的是默认的10，那么当map计算完成做merge时，就没有办法一次将所有的spill文件merge成一个，而是会分多次，每次最多10个stream。这也就是说，当map的中间结果非常大，调大io.sort.factor，有利于减少merge次数，进而减少map对磁盘的读写频率，有可能达到优化作业的目的。</p>

<p>当job指定了combiner的时候，我们都知道map介绍后会在map端根据combiner定义的函数将map结果进行合并。运行combiner函数的时机有可能会是merge完成之前，或者之后，这个时机可以由一个参数控制，即min.num.spill.for.combine（default 3），当job中设定了combiner，并且spill数最少有3个的时候，那么combiner函数就会在merge产生结果文件之前运行。通过这样的方式，就可以在spill非常多需要merge，并且很多数据需要做conbine的时候，减少写入到磁盘文件的数据数量，同样是为了减少对磁盘的读写频率，有可能达到优化作业的目的。</p>

<p>减少中间结果读写进出磁盘的方法不止这些，还有就是压缩。也就是说map的中间，无论是spill的时候，还是最后merge产生的结果文件，都是可以压缩的。压缩的好处在于，通过压缩减少写入读出磁盘的数据量。对中间结果非常大，磁盘速度成为map执行瓶颈的job，尤其有用。控制map中间结果是否使用压缩的参数为：mapred.compress.map.output(true/false)。将这个参数设置为true时，那么map在写中间结果时，就会将数据压缩后再写入磁盘，读结果时也会采用先解压后读取数据。这样做的后果就是：写入磁盘的中间结果数据量会变少，但是cpu会消耗一些用来压缩和解压。所以这种方式通常适合job中间结果非常大，瓶颈不在cpu，而是在磁盘的读写的情况。说的直白一些就是用cpu换IO。根据观察，通常大部分的作业cpu都不是瓶颈，除非运算逻辑异常复杂。所以对中间结果采用压缩通常来说是有收益的。以下是一个wordcount中间结果采用压缩和不采用压缩产生的map中间结果本地磁盘读写的数据量对比：</p>

<ul>
<li><p>map中间结果不压缩：<br/>
<img src="http://luoli523.github.com/static/job_un_codec.jpg" alt="job_un_codec" /></p></li>
<li><p>map中间结果压缩：<br/>
<img src="http://luoli523.github.com/static/job_codec.jpg" alt="job_codec" /></p></li>
</ul>


<p>可以看出，同样的job，同样的数据，在采用压缩的情况下，map中间结果能缩小将近10倍，如果map的瓶颈在磁盘，那么job的性能提升将会非常可观。</p>

<p>当采用map中间结果压缩的情况下，用户还可以选择压缩时采用哪种压缩格式进行压缩，现在hadoop支持的压缩格式有：GzipCodec，LzoCodec，BZip2Codec，LzmaCodec等压缩格式。通常来说，想要达到比较平衡的cpu和磁盘压缩比，LzoCodec比较适合。但也要取决于job的具体情况。用户若想要自行选择中间结果的压缩算法，可以设置配置参数：mapred.map.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec或者其他用户自行选择的压缩方式。</p>

<h3>Map side相关参数调优</h3>

<pre><code>选项                                      类型    默认值         描述
io.sort.mb                                int        100            缓存map中间结果的buffer大小(in MB)
io.sort.record.percent                    float    0.05         io.sort.mb中用来保存map output记录边界的百分比，其他缓存用来保存数据
io.sort.spill.percent                       float      0.80         map开始做spill操作的阈值
io.sort.factor                            int        10           做merge操作时同时操作的stream数上限。
min.num.spill.for.combine                   int      3            combiner函数运行的最小spill数
mapred.compress.map.output                boolean    false        map中间结果是否采用压缩
mapred.map.output.compression.codec       classname org.apache.hadoop.io.compress.DefaultCodec map中间结果的压缩格式
</code></pre>

<h2>Reduce side tuning参数</h2>

<h3>ReduceTask运行内部原理</h3>

<p><img src="http://luoli523.github.com/static/reduceTask.jpg" alt="reduceTask" />
reduce的运行是分成三个阶段的。分别为copy->sort->reduce。由于job的每一个map都会根据reduce(n)数将数据分成map 输出结果分成n个partition，所以map的中间结果中是有可能包含每一个reduce需要处理的部分数据的。所以，为了优化reduce的执行时间，hadoop中是等job的第一个map结束后，所有的reduce就开始尝试从完成的map中下载该reduce对应的partition部分数据。这个过程就是通常所说的shuffle，也就是copy过程。</p>

<p>Reduce task在做shuffle时，实际上就是从不同的已经完成的map上去下载属于自己这个reduce的部分数据，由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载，这个并行度是可以调整的，调整参数为：mapred.reduce.parallel.copies（default 5）。默认情况下，每个只会有5个并行的下载线程在从map下数据，如果一个时间段内job完成的map有100个或者更多，那么reduce也最多只能同时下载5个map的数据，所以这个参数比较适合map很多并且完成的比较快的job的情况下调大，有利于reduce更快的获取属于自己部分的数据。</p>

<p>reduce的每一个下载线程在下载某个map数据的时候，有可能因为那个map中间结果所在机器发生错误，或者中间结果的文件丢失，或者网络瞬断等等情况，这样reduce的下载就有可能失败，所以reduce的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从另外的地方下载（因为这段时间map可能重跑）。所以reduce下载线程的这个最大的下载时间段是可以调整的，调整参数为：mapred.reduce.copy.backoff（default 300秒）。如果集群环境的网络本身是瓶颈，那么用户可以通过调大这个参数来避免reduce下载线程被误判为失败的情况。不过在网络环境比较好的情况下，没有必要调整。通常来说专业的集群网络不应该有太大问题，所以这个参数需要调整的情况不多。</p>

<p>Reduce将map结果下载到本地时，同样也是需要进行merge的，所以io.sort.factor的配置选项同样会影响reduce进行merge时的行为，该参数的详细介绍上文已经提到，当发现reduce在shuffle阶段iowait非常的高的时候，就有可能通过调大这个参数来加大一次merge时的并发吞吐，优化reduce效率。</p>

<p>Reduce在shuffle阶段对下载来的map数据，并不是立刻就写入磁盘的，而是会先缓存在内存中，然后当使用内存达到一定量的时候才刷入磁盘。这个内存大小的控制就不像map一样可以通过io.sort.mb来设定了，而是通过另外一个参数来设置：mapred.job.shuffle.input.buffer.percent（default 0.7），这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。也就是说，如果该reduce task的最大heap使用量（通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</p>

<p>假设mapred.job.shuffle.input.buffer.percent为0.7，reduce task的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右，这700M的内存，跟map端一样，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷。这个限度阈值也是可以通过job参数来设定的，设定参数为：mapred.job.shuffle.merge.percent（default 0.66）。如果下载速度很快，很容易就把内存缓存撑大，那么调整一下这个参数有可能会对reduce的性能有所帮助。</p>

<p>当reduce将所有的map上对应自己partition的数据下载完成后，就会开始真正的reduce计算阶段（中间有个sort阶段通常时间非常短，几秒钟就完成了，因为整个下载阶段就已经是边下载边sort，然后边merge的）。当reduce task真正进入reduce函数的计算阶段的时候，有一个参数也是可以调整reduce的计算行为。也就是：mapred.job.reduce.input.buffer.percent（default 0.0）。由于reduce计算时肯定也是需要消耗内存的，而在读取reduce需要的数据时，同样是需要内存作为buffer，这个参数是控制，需要多少的内存百分比来作为reduce读已经sort好的数据的buffer百分比。默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读处理数据。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，反正reduce的内存闲着也是闲着。</p>

<h3>Reduce side相关参数调优</h3>

<pre><code>选项                                      类型    默认值         描述
mapred.reduce.parallel.copies               int       5         每个reduce并行下载map结果的最大线程数
mapred.reduce.copy.backoff                int        300          reduce下载线程最大等待时间（in sec）
io.sort.factor                            int         10          同上
mapred.job.shuffle.input.buffer.percent   float     0.7         用来缓存shuffle数据的reduce task heap百分比
mapred.job.shuffle.merge.percent            float       0.66        缓存的内存中多少百分比后开始做merge操作
mapred.job.reduce.input.buffer.percent    float     0.0       sort完成后reduce计算阶段用来缓存数据的百分比
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop rpc异步返回机制，大幅降低namenode processTime和queueTime]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-%2Cda-fu-jiang-di-namenode-processtimehe-queuetime/"/>
    <updated>2012-11-04T16:00:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-,da-fu-jiang-di-namenode-processtimehe-queuetime</id>
    <content type="html"><![CDATA[<p>在我们的集群里，namenode是关注度最高的一个地方，尤其是作业运行开始变慢，集群吞吐开始下降的时候，虽然有可能是其他的原因，但第一个被想到的，总是namenode，俨然有一种superstar的感觉。 <br/>
其实这是正常的。虽然只有在集群高负荷运行的时候，namenode的吞吐才会直接影响到整个集群的效率，但是。。。。下面这张图是云梯集群全天24小时map和reduce计算槽位的运行情况: <br/>
<img src="http://luoli523.github.com/static/MR-running.GIF" alt="MR daily running" /></p>

<p>直接一句话就是：除了每天0点到9点的生产时段，云梯集群全天无间隙的满负载运行。所以几乎每天，namenode的吞吐都是集群管理员关注的焦点。毫不夸张的说，我对namenode各项性能指标的熟悉程度，甚至要超过我对我家厨房里碗和调羹的个数和我家冰箱里还剩下多少杯酸奶。</p>

<p>最近做了一个比较大的改动，从测试的数据来看，效果不错。所以把中间的细节整理了一下：</p>

<!-- more -->


<p>namenode服务的时候，其实他运行的方式非常的简单，对文件系统的操作无非就是以下这些： <br/>
1. create<br/>
2. mkdir <br/>
3. rename (mv)<br/>
4. delete<br/>
5. complete (close)<br/>
6. setPermission<br/>
7. getFileInfo<br/>
8. getListing (ls)<br/>
9. &#8230;..</p>

<p>不管是在在传统的文件系统里还是在分布式的文件系统里，对文件系统的操作无非也就是这些。只不过在分布式文件系统里，这些操作都是通过RPC的方式来调用的。所以所白了，namenode做的事情，就是通过RPC Server对来自Client的以上各种请求进行响应而已。所以，抛开内部细节，从一个很上层的角度去看，namenode工作原理就如下图所示（因为这里要讲述的重点不在NN内存结构内部，其中NN内部内存结构的很多细节忽略了）： <br/>
<img src="http://luoli523.github.com/static/nn-no-syncthread.PNG" alt="nn-no-syncthread" /></p>

<p>从上图就可以看出，影响namenode性能的几个点，忽略掉一些细节，从一个很高的层面来看，分别是一下几个地方： <br/>
* Rpc Server接收rpc调用的效率 <br/>
* NN内存结构内在加读锁或者写锁后的处理效率 <br/>
* editLog的sync效率 <br/>
* Rpc的返回效率</p>

<p>在我们的优化之前，namenode处理一个一个rpc的流程就如上图所示，流程如下:</p>

<ol>
<li>Client通过rpc向namenode rpc server发起一个请求（如mkdir）</li>
<li>namenode Rpc Server的listener线程accept这个请求</li>
<li>Listener的子线程Reader从client读取调用函数和参数,并将这些数据抽象成一个Call对象,存在CallQueue中,等待handler处理</li>
<li>Handler线程中某一个空闲的线程从CallQueue中取出一个Call(比如就是刚才的mkdir),然后发现是要加写锁的操作,于是等待NN的WriteLock</li>
<li>拿到WriteLock写锁以后,在NN内部数据结构中创建一个目录(dir)</li>
<li>将这个对namespace的修改记录到(sync)editlog中</li>
<li>释放NN写锁</li>
<li>将这个调用的返回交给Responder线程</li>
<li>Responder线程在获取CPU时间片后向Client返回这次调用(成功or失败)</li>
</ol>


<p>至此,一次rpc的调用过程结束.</p>

<p>从这个过程可以很明显的看出来, 整个环节中,任何一个地方都有可能成为瓶颈, 我们一轮一轮的优化就是在解决了最耗时的地方瓶颈后,瓶颈点转移到另外的地方,然后下一轮接着优化的这样一个过程.  <br/>
这次的修改主要针对的是以下的瓶颈点(以前几次优化针对的是其他的地方,所以我会在接下来的笔记里详细记录):</p>

<p>在第6步，当这种加写锁的操作一旦过多，那么由于每一次加写锁的操作实际上都修改了整个文件系统的namespace，所以为了数据一致性的保证，必须要将这样的一次修改记录在editlog中（相当于mysql中的redo log），而我们的editlog为了保证数据可靠，配置了两个写入点，一个是namenode本地磁盘（12块SAS盘+Raid10卡），一个是通过NFS写入到另外一台远程的机器。当对editlog的sync变多时，由于editlog是顺序写入，那么就导致很多要sync edit的调用都等在这个地方无法交给Responder线程进行返回，也就无法释放NN的写锁。进而拖慢整个namenode的吞吐和响应速度。通常在这种时候，云梯hadoop用户的旺旺群里就会有用户问：“云梯怎么这么慢？又挂了？”</p>

<p>针对这种情况，这次做的优化是这样的一种思路：</p>

<ul>
<li>由于在第6步sync editlog之前，其实整个调用已经可以返回给用户了，但是为了数据可靠性，就必须要等两个editlog的写入sync结束以后才能释放写锁，其他handler才能进一步处理其他的调用。所以，只要能够确保rpc在sync完editlog以后再返回这一点，那么，其实可以把等待sync完成的这段时间用来处理其他的rpc请求，就不会有数据丢失的风险。</li>
<li>也就是说，可以让rpc在完成第5步以后，等待sync完成之前，就将NN的写锁释放，并将sync的等待交给后台线程（SyncThread）去做，并在sync完成以后，将这个rpc的返回交给responder线程，那么就可以把所有rpc调用中等待sync磁盘的时间给释放出来，提供更多的服务。</li>
<li>修改过后，在第6步完成以后，其他的handler就不需要在等待writeLock，可以开始获取写锁开始服务</li>
</ul>


<p>因此，修改以后，整个过程变成如下形式： <br/>
<img src="http://luoli523.github.com/static/nn-syncthread.PNG" alt="nn-syncthread" /></p>

<p>从原理上分析，优化以后肯定会有如下效果：</p>

<ol>
<li>namenode响应中，rpc的processTime肯定会下降（因为没有了等待sync磁盘的时间）</li>
<li>rpc CallQueue队列中的queueTime(每个call在callqueue中的排队时间)肯定也会下降</li>
<li>namenode的整体吞吐肯定会上升</li>
<li>namenode的CPU利用率肯定会上升（等待时间变短）</li>
</ol>


<p>通过测试集群的压测（不得不说，分布式测试团队的同事们实在是给力啊），以下是详细的各项指标的数据变化情况：<br/>
<img src="http://luoli523.github.com/static/nn-syncthread-perf.PNG" alt="nn-syncthread-perf" /></p>

<p>不过这次优化因为一个bug上线回滚了，在线上运行的那段时间观察效果的确很明显，赶上双十一封网，跟团队同事商量后，为了保险起见，决定双十一以后再上线。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[跟facebook工程师交流HDFS笔记整理]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li/"/>
    <updated>2012-11-03T17:17:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li</id>
    <content type="html"><![CDATA[<p>hadoop在纽约的大会今年是10月22日～10月25日召开的。被公司派去美国参加hadoop大会，本来非常高兴，谁知道一切具备的情况下居然莫名其妙的被美国使馆给拒签了…… 连B1签证都被拒了，点儿实在是太背。没办法，本来事先约好的跟facebook hadoop团队的交流中我的那部分，只有由同事代劳了。听说FB的人得知我因为签证被拒而没有去，都很诡异的笑了 -_-</p>

<p>不过还好，虽然人没到场，但我要交流的topic和相应的细节都整理好了托同事带到了，然后通过事后的邮件来往，并不影响实质的交流。随着沟通的深入，我们的情况他们也都了解，他们的一些细节，我也都已经清楚。</p>

<p>很客观的讲，在开发方面（运维方面我们的ops团队实在是够专业：） <a href="http://weibo.com/u/1804480064">@淘大舞</a> <a href="http://weibo.com/u/1084192524">@dun_2010</a>），至少在存储这一部分，FB比我们做的好，走的比我们快。邮件的深入交流中，我整理了一下他们特别提到的几点。应该说，由于集群规模和数据规模很接近，使用的hadoop版本也接近，所以大家遇到的问题和解决的方式都基本是很接近的思路，这其实非常的神奇，他们自己都说：在另外的一个地方，有一个跟我们最大的集群差不多规模的集群，做着相同的事情，遇到相同的问题，真的是一件神奇的事情。</p>

<p>把交流的内容整理了一下，主要有一下几个方面：</p>

<!-- more -->


<h3>namenode heapsize</h3>

<p>这个问题我们和FB都遇到了，而且很客观的讲，全球所有的hadoop集群，碰到namenode heapSize触发java6 JDK bug的，可能就只有FB的warehouse集群和我们的云梯集群。这个bug导致的后果就是namenode的heapsize加大到130GB以后，再往上加就会crash，即使物理内存够（我们服务器的是192GB）也无法使用了。想要遇到这个问题，其实不是一件容易的事情，集群的规模没到，前期各种瓶颈没有解决，是不可能来到这个地方。不过对FB来说，他们的Federation已经在线上运行，所以遇到这个问题可以绕过去，而且他们已经在调研java7，估计在不久后会迁移上去（BTW，java7中这个问题的解决也是淘宝同事的贡献：））。对我们来说非常的不幸，由于federation还在开发阶段，就不得不直接面对。幸好我们的JVM组的同事给力（<a href="http://weibo.com/u/1920312980">@王王争</a> <a href="http://weibo.com/halmo">@坤谷</a>），拿到我们的反馈后，迅速做出响应，并将这个bug的解决patch提交给了java社区（详情请见 <a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7197906">bug详情</a> 和 <a href="http://cr.openjdk.java.net/~brutisso/7197906/webrev.02/">patch</a> BTW：我永远不会承认java跟Oracle这家律师事务所有任何的关系，SUN才是Java的娘家），问题解决。<br/>
<strong>如果你走在雷区的最前沿，趟雷的是你，为别人插旗指路的也是你。</strong></p>

<h3>储存优化</h3>

<p>由于namenode heapsize的问题，引发了很多的讨论，各自也都想过很多的办法。从开发上，FB开发了一套HDFS Raid系统，能够在不降低数据可靠性的情况下，利用Reed Solomon编码（RS编码），减少分布式文件对物理存储20%～30%的消耗。从Raid系统核心开发人员的沟通中了解到，Raid系统已经帮助FB节省了10.74 PB的物理存储空间。可能有人对这个没有太深的概念，但是，十几个PB的存储空间，几百万的资产。。。。 <br/>
经过几个月对代码的熟悉和修改，我们的Raid系统也与今年7月份成功上线，目前节省存储空间100T左右。我们的Raid系统才刚刚起步，刚上线不久，公司很多业务线都还不了解，还没有把自己的数据给Raid化，不过相信我们不遗余力的推广，成效会慢慢显现出来的。（这里其实不得不说，有时候在一些机构推行一个对大家都有利的东西，或者一个机制，真的好难。就好比给绝大部分的人两个选择：1，伴随着阵痛的治疗；2，无痛的死去，面对这样的选择，大部分的人居然会选择后者……原因很多，但有一点不可否认：绩效，让很多人失去闯劲和改变的勇气。治疗？改变？可没有KPI里的业绩重要。。。。不得不感谢量子团队和数据平台团队勇敢的成为第一个吃螃蟹的人，积极的配合我们去优化存储，造福大家。）越说越远了。。。赶紧拉回来！</p>

<h3>设计上的修改</h3>

<p>FB的同学还透露给我目前还在思考的一个从开发上的改进方案：<strong>可变blocksize方案。</strong> 目前在HDFS中，对一个文件进行切分的时候，都是按照固定blocksize进行切分，除了最后一个block，其他的block的size都是一样大。而如果实现了可变blocksize方案，这个格局将被打破，文件的不同block，size可以不一样（这一点其实不难做到，因为在namenode里，每个block对象的size其实都是一个独立的变量）。这样的后果就是：当要合并两个文件的时候，不再需要像现在一样将两个文件进行读出，然后顺序的写到另外一个merge的文件中去。也就是说，要合并文件，只需要一个调用，修改一点点meta信息，不产生任何的IO读写就能够做到。这将是一个不错的选择，不错的方案，能够以最小的代价合并HDFS上的小文件，减少meta信息对namenode内存的消耗。concatenate files，这将是我们团队接下来的一个很重要的工作。我会把细节和原理在将来的笔记里记录到这里来。</p>

<h3>RPC方面</h3>

<p>FB的做法跟我们一样，因为他们也发现了在namenode的个总rpc响应中，每一个rpc的处理时间里有很大的一部分是用来处理文件路径（breaking into components, getting bytes out of them），所以对namenode优化很大的一部分就是将这种类型的操作尽量的挪出读写锁。另外，FB的同学提到了一个新的想法：将namenode的rpc中，处理datanode的rpc和来自Client的rpc分开，用100个handler来处理datanode的请求，用更多的handler来处理Client的请求，根据他们的介绍，这能够对提高namendoe吞吐有很大的帮助。这是我们还没有尝试过的做法，肯定从程序的实现上做了比较大的修改，还有很多细节需要去了解才行。</p>

<h3>edits log</h3>

<p>目前FB也是使用了本地和NFS各一份的方式，不过也在做Bookkeeper和QJM的调研，呵呵，真的很神奇，大家的想法再一次不谋而合。</p>

<h3>Federation</h3>

<p>FB已经上线了他们的federation，实现跟社区稍有不一样，但原理基本一致。而且他们也是一个namenode一个pool的做法，这样简单。<strong>设计上完美的东西，永远都只能停留在paper和PPT里面，真正解决问题的，通常都是那些简单可靠的办法。</strong>（非常遗憾，在美国人看来，这样的做法简单高效，是最优方案，在很多中国人看来。。。。。 还不够吹毛求疵的。唉，又扯远了）</p>

<h3>Failover</h3>

<p>还有一点也非常神奇，FB的failover方案，目前来说也是人工介入的切换，并没有要做成自动切换。说白了，failover要解决的问题，99%都是升级不停服务的问题，为了TNND那1%的可能性去做那80%的工作，真的没有必要.</p>

<p>整理了一下，重要的就以上这些。真的是对我们接下来的工作有很多方向上的帮助。不得不说，跟FB的开发团队的交流有一种如沐春风的感觉。很单纯的一群人，听说他们的Raid系统在我们的集群也成功上线，他们都高兴的欢呼，击掌相庆。什么叫价值观？这才是价值观:)</p>

<p>自从删除CSDN上的博客以后，好久没有整理笔记了，以后我会把我们开发中的积累和经验都记下来，整理到这里来。呵呵，好记性，还是不如烂笔头的～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[新博客，第一篇，写给云梯]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-%2Cdi-%5B%3F%5D-pian-%2Cxie-gei-yun-ti/"/>
    <updated>2012-11-03T00:19:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-,di-[?]-pian-,xie-gei-yun-ti</id>
    <content type="html"><![CDATA[<p>“数大就是美”，这是徐志摩同学《志摩日记二则》中的一句话：“数大”便是美。比如说：从无极的蓝空中下窥大地，是美；泰山顶上的云海，高大的云峰在晨光里镇定着，是美……。其实不仅仅这些，对于我来说，从云梯集群300台机器涨到现在3000多台，是美；云梯namenode的rpc吞吐一次次的上涨，是美；云梯的每一次性能数据的提升，也是美。</p>

<p>虽然过程并不是非常顺利，虽然伴随着各种惊心动魄和提心吊胆，虽然最新的一次升级经历了两次失败，但是昨天（2012-11-01），我们的云梯集群第一次超过了3000规模（3160），存储容量达到65PB，namenode平均每天处理rpc数量20+亿，集群DFS中文件和目录上2.2亿，blocks数2.8亿。到这种程度，我们所做的任何事，早已经超越了一般的程序设计和技术开发，每一次的升级，每一行代码的修改，都牵动着整个公司几千工程师和他们的工作，他们的业绩，和他们对我们的信心。每动一下手指，都是责任。对我一个小小的工程师来说，这样一份天大的责任，扛的实在是诚恐诚惶。最近的两次升级都失败了，引来了很多的抱怨和质疑，很对不起那些翘首企盼的同事们。不知道该说什么好，我能做的：尽人事，听天命。</p>

<p>新博客开张，第一篇，写给云梯。加油吧</p>

<p>BTW：感谢华仔帮我申请的域名，我想我会一直保留这个域名的。</p>
]]></content>
  </entry>
  
</feed>
