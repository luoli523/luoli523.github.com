<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[luoli523]]></title>
  <link href="http://luoli523.github.com/atom.xml" rel="self"/>
  <link href="http://luoli523.github.com/"/>
  <updated>2012-11-04T16:33:39+08:00</updated>
  <id>http://luoli523.github.com/</id>
  <author>
    <name><![CDATA[罗李]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hadoop rpc异步返回机制，大幅降低namenode processTime和queueTime]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-%2Cda-fu-jiang-di-namenode-processtimehe-queuetime/"/>
    <updated>2012-11-04T16:00:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-,da-fu-jiang-di-namenode-processtimehe-queuetime</id>
    <content type="html"><![CDATA[<p>在我们的集群里，namenode是关注度最高的一个地方，尤其是作业运行开始变慢，集群吞吐开始下降的时候，虽然有可能是其他的原因，但第一个被想到的，总是namenode，俨然有一种superstar的感觉。 <br/>
其实这是正常的。虽然只有在集群高负荷运行的时候，namenode的吞吐才会直接影响到整个集群的效率，但是。。。。下面这张图是云梯集群全天24小时map和reduce计算槽位的运行情况: <br/>
<img src="http://luoli523.github.com/static/MR-running.GIF" alt="MR daily running" /></p>

<p>直接一句话就是：除了每天0点到9点的生产时段，云梯集群全天无间隙的满负载运行。所以几乎每天，namenode的吞吐都是集群管理员关注的焦点。毫不夸张的说，我对namenode各项性能指标的熟悉程度，甚至要超过我对我家厨房里碗和调羹的个数和我家冰箱里还剩下多少杯酸奶。</p>

<p>最近做了一个比较大的改动，从测试的数据来看，效果不错。所以把中间的细节整理了一下：</p>

<!-- more -->


<p>namenode服务的时候，其实他运行的方式非常的简单，对文件系统的操作无非就是以下这些： <br/>
1. create<br/>
2. mkdir <br/>
3. rename (mv)<br/>
4. delete<br/>
5. complete (close)<br/>
6. setPermission<br/>
7. getFileInfo<br/>
8. getListing (ls)<br/>
9. &#8230;..</p>

<p>不管是在在传统的文件系统里还是在分布式的文件系统里，对文件系统的操作无非也就是这些。只不过在分布式文件系统里，这些操作都是通过RPC的方式来调用的。所以所白了，namenode做的事情，就是通过RPC Server对来自Client的以上各种请求进行响应而已。所以，抛开内部细节，从一个很上层的角度去看，namenode工作原理就如下图所示（因为这里要讲述的重点不在NN内存结构内部，其中NN内部内存结构的很多细节忽略了）： <br/>
<img src="http://luoli523.github.com/static/nn-no-syncthread.PNG" alt="nn-no-syncthread" /></p>

<p>从上图就可以看出，影响namenode性能的几个点，忽略掉一些细节，从一个很高的层面来看，分别是一下几个地方： <br/>
* Rpc Server接收rpc调用的效率 <br/>
* NN内存结构内在加读锁或者写锁后的处理效率 <br/>
* editLog的sync效率 <br/>
* Rpc的返回效率</p>

<p>在我们的优化之前，namenode处理一个一个rpc的流程就如上图所示，流程如下:</p>

<ol>
<li>Client通过rpc向namenode rpc server发起一个请求（如mkdir）</li>
<li>namenode Rpc Server的listener线程accept这个请求</li>
<li>Listener的子线程Reader从client读取调用函数和参数,并将这些数据抽象成一个Call对象,存在CallQueue中,等待handler处理</li>
<li>Handler线程中某一个空闲的线程从CallQueue中取出一个Call(比如就是刚才的mkdir),然后发现是要加写锁的操作,于是等待NN的WriteLock</li>
<li>拿到WriteLock写锁以后,在NN内部数据结构中创建一个目录(dir)</li>
<li>将这个对namespace的修改记录到(sync)editlog中</li>
<li>释放NN写锁</li>
<li>将这个调用的返回交给Responder线程</li>
<li>Responder线程在获取CPU时间片后向Client返回这次调用(成功or失败)</li>
</ol>


<p>至此,一次rpc的调用过程结束.</p>

<p>从这个过程可以很明显的看出来, 整个环节中,任何一个地方都有可能成为瓶颈, 我们一轮一轮的优化就是在解决了最耗时的地方瓶颈后,瓶颈点转移到另外的地方,然后下一轮接着优化的这样一个过程.  <br/>
这次的修改主要针对的是以下的瓶颈点(以前几次优化针对的是其他的地方,所以我会在接下来的笔记里详细记录):</p>

<p>在第6步，当这种加写锁的操作一旦过多，那么由于每一次加写锁的操作实际上都修改了整个文件系统的namespace，所以为了数据一致性的保证，必须要将这样的一次修改记录在editlog中（相当于mysql中的redo log），而我们的editlog为了保证数据可靠，配置了两个写入点，一个是namenode本地磁盘（12块SAS盘+Raid10卡），一个是通过NFS写入到另外一台远程的机器。当对editlog的sync变多时，由于editlog是顺序写入，那么就导致很多要sync edit的调用都等在这个地方无法交给Responder线程进行返回，也就无法释放NN的写锁。进而拖慢整个namenode的吞吐和响应速度。通常在这种时候，云梯hadoop用户的旺旺群里就会有用户问：“云梯怎么这么慢？又挂了？”</p>

<p>针对这种情况，这次做的优化是这样的一种思路：</p>

<ul>
<li>由于在第6步sync editlog之前，其实整个调用已经可以返回给用户了，但是为了数据可靠性，就必须要等两个editlog的写入sync结束以后才能释放写锁，其他handler才能进一步处理其他的调用。所以，只要能够确保rpc在sync完editlog以后再返回这一点，那么，其实可以把等待sync完成的这段时间用来处理其他的rpc请求，就不会有数据丢失的风险。</li>
<li>也就是说，可以让rpc在完成第5步以后，等待sync完成之前，就将NN的写锁释放，并将sync的等待交给后台线程（SyncThread）去做，并在sync完成以后，将这个rpc的返回交给responder线程，那么就可以把所有rpc调用中等待sync磁盘的时间给释放出来，提供更多的服务。</li>
<li>修改过后，在第6步完成以后，其他的handler就不需要在等待writeLock，可以开始获取写锁开始服务</li>
</ul>


<p>因此，修改以后，整个过程变成如下形式： <br/>
<img src="http://luoli523.github.com/static/nn-syncthread.PNG" alt="nn-syncthread" /></p>

<p>从原理上分析，优化以后肯定会有如下效果：</p>

<ol>
<li>namenode响应中，rpc的processTime肯定会下降（因为没有了等待sync磁盘的时间）</li>
<li>rpc CallQueue队列中的queueTime(每个call在callqueue中的排队时间)肯定也会下降</li>
<li>namenode的整体吞吐肯定会上升</li>
<li>namenode的CPU利用率肯定会上升（等待时间变短）</li>
</ol>


<p>通过测试集群的压测（不得不说，分布式测试团队的同事们实在是给力啊），以下是详细的各项指标的数据变化情况：<br/>
<img src="http://luoli523.github.com/static/nn-syncthread-perf.PNG" alt="nn-syncthread-perf" /></p>

<p>不过这次优化因为一个bug上线回滚了，在线上运行的那段时间观察效果的确很明显，赶上双十一封网，跟团队同事商量后，为了保险起见，决定双十一以后再上线。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[跟facebook工程师交流HDFS笔记整理]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li/"/>
    <updated>2012-11-03T17:17:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li</id>
    <content type="html"><![CDATA[<p>hadoop在纽约的大会今年是10月22日～10月25日召开的。被公司派去美国参加hadoop大会，本来非常高兴，谁知道一切具备的情况下居然莫名其妙的被美国使馆给拒签了…… 连B1签证都被拒了，点儿实在是太背。没办法，本来事先约好的跟facebook hadoop团队的交流中我的那部分，只有由同事代劳了。听说FB的人得知我因为签证被拒而没有去，都很诡异的笑了 -_-</p>

<p>不过还好，虽然人没到场，但我要交流的topic和相应的细节都整理好了托同事带到了，然后通过事后的邮件来往，并不影响实质的交流。随着沟通的深入，我们的情况他们也都了解，他们的一些细节，我也都已经清楚。</p>

<p>很客观的讲，在开发方面（运维方面我们的ops团队实在是够专业：） <a href="http://weibo.com/u/1804480064">@淘大舞</a> <a href="http://weibo.com/u/1084192524">@dun_2010</a>），至少在存储这一部分，FB比我们做的好，走的比我们快。邮件的深入交流中，我整理了一下他们特别提到的几点。应该说，由于集群规模和数据规模很接近，使用的hadoop版本也接近，所以大家遇到的问题和解决的方式都基本是很接近的思路，这其实非常的神奇，他们自己都说：在另外的一个地方，有一个跟我们最大的集群差不多规模的集群，做着相同的事情，遇到相同的问题，真的是一件神奇的事情。</p>

<p>把交流的内容整理了一下，主要有一下几个方面：</p>

<!-- more -->


<h3>namenode heapsize</h3>

<p>这个问题我们和FB都遇到了，而且很客观的讲，全球所有的hadoop集群，碰到namenode heapSize触发java6 JDK bug的，可能就只有FB的warehouse集群和我们的云梯集群。这个bug导致的后果就是namenode的heapsize加大到130GB以后，再往上加就会crash，即使物理内存够（我们服务器的是192GB）也无法使用了。想要遇到这个问题，其实不是一件容易的事情，集群的规模没到，前期各种瓶颈没有解决，是不可能来到这个地方。不过对FB来说，他们的Federation已经在线上运行，所以遇到这个问题可以绕过去，而且他们已经在调研java7，估计在不久后会迁移上去（BTW，java7中这个问题的解决也是淘宝同事的贡献：））。对我们来说非常的不幸，由于federation还在开发阶段，就不得不直接面对。幸好我们的JVM组的同事给力（<a href="http://weibo.com/u/1920312980">@王王争</a> <a href="http://weibo.com/halmo">@坤谷</a>），拿到我们的反馈后，迅速做出响应，并将这个bug的解决patch提交给了java社区（详情请见 <a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7197906">bug详情</a> 和 <a href="http://cr.openjdk.java.net/~brutisso/7197906/webrev.02/">patch</a> BTW：我永远不会承认java跟Oracle这家律师事务所有任何的关系，SUN才是Java的娘家），问题解决。<br/>
<strong>如果你走在雷区的最前沿，趟雷的是你，为别人插旗指路的也是你。</strong></p>

<h3>储存优化</h3>

<p>由于namenode heapsize的问题，引发了很多的讨论，各自也都想过很多的办法。从开发上，FB开发了一套HDFS Raid系统，能够在不降低数据可靠性的情况下，利用Reed Solomon编码（RS编码），减少分布式文件对物理存储20%～30%的消耗。从Raid系统核心开发人员的沟通中了解到，Raid系统已经帮助FB节省了10.74 PB的物理存储空间。可能有人对这个没有太深的概念，但是，十几个PB的存储空间，几百万的资产。。。。 <br/>
经过几个月对代码的熟悉和修改，我们的Raid系统也与今年7月份成功上线，目前节省存储空间100T左右。我们的Raid系统才刚刚起步，刚上线不久，公司很多业务线都还不了解，还没有把自己的数据给Raid化，不过相信我们不遗余力的推广，成效会慢慢显现出来的。（这里其实不得不说，有时候在一些机构推行一个对大家都有利的东西，或者一个机制，真的好难。就好比给绝大部分的人两个选择：1，伴随着阵痛的治疗；2，无痛的死去，面对这样的选择，大部分的人居然会选择后者……原因很多，但有一点不可否认：绩效，让很多人失去闯劲和改变的勇气。治疗？改变？可没有KPI里的业绩重要。。。。不得不感谢量子团队和数据平台团队勇敢的成为第一个吃螃蟹的人，积极的配合我们去优化存储，造福大家。）越说越远了。。。赶紧拉回来！</p>

<h3>设计上的修改</h3>

<p>FB的同学还透露给我目前还在思考的一个从开发上的改进方案：<strong>可变blocksize方案。</strong> 目前在HDFS中，对一个文件进行切分的时候，都是按照固定blocksize进行切分，除了最后一个block，其他的block的size都是一样大。而如果实现了可变blocksize方案，这个格局将被打破，文件的不同block，size可以不一样（这一点其实不难做到，因为在namenode里，每个block对象的size其实都是一个独立的变量）。这样的后果就是：当要合并两个文件的时候，不再需要像现在一样将两个文件进行读出，然后顺序的写到另外一个merge的文件中去。也就是说，要合并文件，只需要一个调用，修改一点点meta信息，不产生任何的IO读写就能够做到。这将是一个不错的选择，不错的方案，能够以最小的代价合并HDFS上的小文件，减少meta信息对namenode内存的消耗。concatenate files，这将是我们团队接下来的一个很重要的工作。我会把细节和原理在将来的笔记里记录到这里来。</p>

<h3>RPC方面</h3>

<p>FB的做法跟我们一样，因为他们也发现了在namenode的个总rpc响应中，每一个rpc的处理时间里有很大的一部分是用来处理文件路径（breaking into components, getting bytes out of them），所以对namenode优化很大的一部分就是将这种类型的操作尽量的挪出读写锁。另外，FB的同学提到了一个新的想法：将namenode的rpc中，处理datanode的rpc和来自Client的rpc分开，用100个handler来处理datanode的请求，用更多的handler来处理Client的请求，根据他们的介绍，这能够对提高namendoe吞吐有很大的帮助。这是我们还没有尝试过的做法，肯定从程序的实现上做了比较大的修改，还有很多细节需要去了解才行。</p>

<h3>edits log</h3>

<p>目前FB也是使用了本地和NFS各一份的方式，不过也在做Bookkeeper和QJM的调研，呵呵，真的很神奇，大家的想法再一次不谋而合。</p>

<h3>Federation</h3>

<p>FB已经上线了他们的federation，实现跟社区稍有不一样，但原理基本一致。而且他们也是一个namenode一个pool的做法，这样简单。<strong>设计上完美的东西，永远都只能停留在paper和PPT里面，真正解决问题的，通常都是那些简单可靠的办法。</strong>（非常遗憾，在美国人看来，这样的做法简单高效，是最优方案，在很多中国人看来。。。。。 还不够吹毛求疵的。唉，又扯远了）</p>

<h3>Failover</h3>

<p>还有一点也非常神奇，FB的failover方案，目前来说也是人工介入的切换，并没有要做成自动切换。说白了，failover要解决的问题，99%都是升级不停服务的问题，为了TNND那1%的可能性去做那80%的工作，真的没有必要.</p>

<p>整理了一下，重要的就以上这些。真的是对我们接下来的工作有很多方向上的帮助。不得不说，跟FB的开发团队的交流有一种如沐春风的感觉。很单纯的一群人，听说他们的Raid系统在我们的集群也成功上线，他们都高兴的欢呼，击掌相庆。什么叫价值观？这才是价值观:)</p>

<p>自从删除CSDN上的博客以后，好久没有整理笔记了，以后我会把我们开发中的积累和经验都记下来，整理到这里来。呵呵，好记性，还是不如烂笔头的～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[新博客，第一篇，写给云梯]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-%2Cdi-%5B%3F%5D-pian-%2Cxie-gei-yun-ti/"/>
    <updated>2012-11-03T00:19:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-,di-[?]-pian-,xie-gei-yun-ti</id>
    <content type="html"><![CDATA[<p>“数大就是美”，这是徐志摩同学《志摩日记二则》中的一句话：“数大”便是美。比如说：从无极的蓝空中下窥大地，是美；泰山顶上的云海，高大的云峰在晨光里镇定着，是美……。其实不仅仅这些，对于我来说，从云梯集群300台机器涨到现在3000多台，是美；云梯namenode的rpc吞吐一次次的上涨，是美；云梯的每一次性能数据的提升，也是美。</p>

<p>虽然过程并不是非常顺利，虽然伴随着各种惊心动魄和提心吊胆，虽然最新的一次升级经历了两次失败，但是昨天（2012-11-01），我们的云梯集群第一次超过了3000规模（3160），存储容量达到65PB，namenode平均每天处理rpc数量20+亿，集群DFS中文件和目录上2.2亿，blocks数2.8亿。到这种程度，我们所做的任何事，早已经超越了一般的程序设计和技术开发，每一次的升级，每一行代码的修改，都牵动着整个公司几千工程师和他们的工作，他们的业绩，和他们对我们的信心。每动一下手指，都是责任。对我一个小小的工程师来说，这样一份天大的责任，扛的实在是诚恐诚惶。最近的两次升级都失败了，引来了很多的抱怨和质疑，很对不起那些翘首企盼的同事们。不知道该说什么好，我能做的：尽人事，听天命。</p>

<p>新博客开张，第一篇，写给云梯。加油吧</p>

<p>BTW：感谢华仔帮我申请的域名，我想我会一直保留这个域名的。</p>
]]></content>
  </entry>
  
</feed>
