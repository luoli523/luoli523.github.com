<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[luoli523]]></title>
  <link href="http://luoli523.github.com/atom.xml" rel="self"/>
  <link href="http://luoli523.github.com/"/>
  <updated>2012-11-26T17:01:38+08:00</updated>
  <id>http://luoli523.github.com/</id>
  <author>
    <name><![CDATA[罗李]]></name>
    <email><![CDATA[luoli523@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[datanode下线对集群带宽影响调研]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/14/datanodexia-xian-dui-ji-qun-dai-kuan-ying-xiang-diao-yan/"/>
    <updated>2012-11-14T22:13:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/14/datanodexia-xian-dui-ji-qun-dai-kuan-ying-xiang-diao-yan</id>
    <content type="html"><![CDATA[<p>这是我两年前做的一个调研和测试，数据和集群规模有点老了，但是结论是有参考价值的。供大家参考。</p>

<h3>背景介绍</h3>

<p>在hadoop集群中，当一个datanode发生故障（宕机，进程被kill，网络不通等）时，namenode在一定时间内（默认10分30秒）无法收到该datanode的心跳信息，就会将该datanode从集群中下线。这样带来的影响是，保存在这台datanode上的所有block副本就需要复制到其他的datanode上去，以保证这些block的冗余副本数满足其期望的副本数（默认3）。 <br/>
按照以上逻辑，一台datanode的下线肯定会造成集群内部各个datanode之间互相的复制block的情况，进而会带来一定的集群内部网络带宽消耗，虽然很多人都知道这个逻辑，但是具体到实际情况下，一台datanode下线对集群的网络带宽影响是一个什么样的程度，HDFS到底是怎么样进行block复制的，是否有带宽限制，如何进行限速配置等等，这些具体问题一直没有非常明确的数据。本次调研就是为了解决这个问题，对datanode下线对HDFS集群的带宽影响做一个比较详细的评估和测试。</p>

<h3>Datanode下线逻辑</h3>

<p>一台datanode下线的逻辑流程如下图所示：</p>

<!-- more -->


<p><img src="http://luoli523.github.com/static/dn-decom/datanode-decommission.PNG" alt="datanode-decommission" /></p>

<p>上图步骤详细操作如下：</p>

<ol>
<li>Datanode1发生异常，无法正常服务，该datanode1往namenode的heartbeat中断（这一过程还有一种可能性是集群管理员通过dfsadmin –refreshNode对某台datanode进行人为的decommission下线，这种情况并不是datanode发生异常，而是管理员的人为调整，但原理是一样）</li>
<li>namendoe内部现成发现datanode1的heartbeat过期（或是管理员人为对某个datanode进行了下线），从namenode内部对datanode1进行下线</li>
<li>namenode从内部数据结构中获取保存在datanode1上的所有blocks的id号，假设名为dn1blocks。</li>
<li>namenode遍历dn1blocks，对每一个block，都选择一个srcNode（保存有该block一个副本的datanode）和一个destNode（没有保存该block副本的一个datanode），然后发送copyBlock命令到srcNode</li>
<li>srcNode将该block拷贝到destNode</li>
</ol>


<p>从以上过程可以看出，实际上，当HDFS下线一台datanode的时候，对网络带宽的消耗主要来自<strong>datanode之间相互的block拷贝</strong>过程.</p>

<h3>Datanode之间block拷贝逻辑</h3>

<p>Datanode之间的block相互拷贝逻辑如下： <br/>
<img src="http://luoli523.github.com/static/dn-decom/datanode-copyblock.PNG" alt="datanode-copyblock" /></p>

<p>从上图可以看出，datanode之间做block相互拷贝的途径有两个：</p>

<ol>
<li><p>在数据写入的pipeline中从一个datanode向下一个datanode发送block数据。这条路径中，Datanode的内部服务线程DataXceiverServer在启动的时候就初始化了一个叫做balanceThrottler 的BlockBalanceThrottler类型变量，这个变量主要作用是用来控制datanode在做相互之间的block拷贝时对带宽的限制。 <br/>
<img src="http://luoli523.github.com/static/dn-decom/code1.PNG" alt="code1" /></p>

<p>   Datanode接到OP_COPY_BLOCK命令后，就会将本机上的block通过网络拷贝到另外一台datanode上去（destNode），而每一次进行copyBlock时，在block数据的传输过程中，都会通过balanceThrottler变量来对copyBlock造成的数据流量和copyBlock线程数进行限制。只有满足以下条件，copyBlock才能顺利执行：</p>

<ol>
<li><strong>当前进行block copy的线程数不能大于5个 </strong></li>
<li><strong>一定时间周期内从该datanode传输数据的速率不能超过一定的阈值</strong>（dfs.balance.bandwidthPerSec，默认1MB/s）<br/>
如果不同时满足以上条件，那么该datanode在一定时间周期内就不会进行block copy的数据传输。</li>
</ol>
</li>
<li><p>另一条block copy的路径为：从Namenode接收Block Transfer cmd，然后Datanode launch一个DataTransfer线程来从该datanode发送block到另外一个datanode。如上图Datanode下方的DataTransfer所示。<strong>从这条路径发起的block拷贝从代码中是没有受到上述balanceThrottler的网络流量限制的。</strong> <br/>
<img src="http://luoli523.github.com/static/dn-decom/code2.PNG" alt="code2" /></p>

<p>   而在集群中，datanode下线的时候，实际上将下线DN上的blocks拷贝到其他datanodes上的操作走的是上述的第二条路径（包括集群管理员人为的通过refreshNode对某台datanode进行人为下线的情况），也就是说：<strong>datanode下线后进行的block copy过程时，bandwidthPerSec的带宽限制并没有生效。因此，Datanode下线，是有可能引起集群大量内部数据拷贝，造成网络带宽激增的。</strong></p></li>
</ol>


<h3>模拟测试</h3>

<p>本测试的目的在于：评估在datanode下线的情况下，真实的集群环境中，会对集群内部带宽带来多大的影响，balanceThrottler的配置限制是否起了作用，是否会造成集群内部的网络block拷贝风暴，是否会影响HDFS的正常服务。 <br/>
本次测试模拟线上集群的环境，云梯HDFS目前的现状如下：<br/>
-  live datanode：1996(两年前的测试，当时云梯才2000台) <br/>
-  平均每台datanode包含blocks数：219127 <br/>
模拟集群环境为：<br/>
-  live datanode：20 <br/>
-  平均每台datanode包含blocks数：10932
<img src="http://luoli523.github.com/static/dn-decom/live-node.PNG" alt="live-node" /></p>

<p>根据云梯HDFS环境和测试集群HDFS环境的等比换算，测试集群上每个datanode应该包含的blocks数应该是2195个，这里模拟datanode下线时，每台datanode平均包含的blocks数要远大于2195，这样可以更好的评估当下线datanode包含大量blocks时，对集群网络带宽造成的影响。</p>

<h4>测试方式</h4>

<p>模拟方式如下：</p>

<ol>
<li>往测试集群的HDFS中灌入数据，观察数据大量写入时的网络情况和各种性能</li>
<li>数据灌入并稳定后，将其中一台datanode停掉，并等待该datanode从namenode中下线</li>
<li>当namenode下线datanode时，开始每隔一秒对namenode进行dfsadmin –metasave操作，记录下每一秒钟内namenode内部的needed replication blocks数和pending replication blocks数，以此来确定每秒钟集群完成多少个blocks的copy。</li>
<li>同时记录在copy blocks时各个datanode的网络带宽占用情况，看是否超过了balanceThrottler设定的阈值</li>
<li>调整balanceThrottler，重复以上过程，确认balanceThrottler在datanode下线的block拷贝中并没有起到限流作用。</li>
</ol>


<h4>运行结果</h4>

<h5>16MBlockSize balanceThrottler 10M/s</h5>

<ul>
<li>blocksize：16MB</li>
<li>balanceThrottler：10MB/s <br/>
下线测试集群中的dw1，包含blocks数为22501个，过程数据记录如下：</li>
<li>过程耗时：35分27秒 (11:38:08~12:13:35)</li>
<li>namenode内部block调度情况：<br/>
从namenode每秒钟的metasave日志可以发现，每一秒钟还有多少个blocks等待进行copy transfer的详细记录。节选日志如下：</li>
</ul>


<p><img src="http://luoli523.github.com/static/dn-decom/network1.PNG" alt="network1" /></p>

<p>统计该metasave日志，结果如下：<br/>
-  整个block transfer过程：<strong>2127</strong>秒 <br/>
-  拷贝blocks数：<strong>22501</strong>个</p>

<p>datanode网络流量监控：
以下是下线datanode以外的其他datanode在block transfer过程中的网络数据流量监控图：</p>

<p><img src="http://luoli523.github.com/static/dn-decom/net-monitor1.PNG" alt="net-monitor1" />
<img src="http://luoli523.github.com/static/dn-decom/net-monitor2.PNG" alt="net-monitor2" /></p>

<p>其他datanode基本相似。从以上各datanode在block拷贝的过程中可以很明显的看出， <strong>由于blocksize只有16M，基本上每个block在一秒多钟左右就能传输完，且由于namenode中的配置选项dfs.max-repl-streams是默认的2，也就是说同一时间namenode只能向datanode发送2个block的copy命令，整个block copy过程消耗的带宽基本稳定在10MB/s上下。</strong></p>

<p>另外，通过对datanode上用来做数据传输的网卡本身流量进行监控，监控日志数据节选如下：</p>

<p><img src="http://luoli523.github.com/static/dn-decom/network2.PNG" alt="network2" /></p>

<p>以上日志是通过在其中一台正常服务的datanode上通过监控程序获取的bond0网卡的流量日志，每一行之间间隔一秒钟，统计结果如下：<br/>
<img src="http://luoli523.github.com/static/dn-decom/network3.PNG" alt="network3" /></p>

<p><strong>从以上统计也可以非常明显的看出，在做block copy的这段时间内，bond0网卡的input KB/s流量为：10261.6 kBytes/s，output KB/s 流量为：9887 kBytes/s，跟10MB/s的datanode block copy网络流量限速基本吻合。</strong></p>

<h5>64MBlockSize balanceThrottler 20M/s</h5>

<ul>
<li>blocksize：64MB</li>
<li>balanceThrottler：20MB/s <br/>
下线测试集群中的dw1，包含blocks数为5791个，过程数据记录如下：<br/>
过程耗时：9分20秒（17:56:59～18:06:19）<br/>
namenode内部block调度情况：<br/>
从namenode每秒钟的metasave日志可以发现，每一秒钟还有多少个blocks等待进行copy transfer的详细记录。节选日志如下：</li>
</ul>


<p><img src="http://luoli523.github.com/static/dn-decom/network4.PNG" alt="network4" /></p>

<p>统计该metasave日志，结果如下：<br/>
整个block transfer过程：<strong>560</strong>秒 <br/>
拷贝blocks数：<strong>5791</strong>个</p>

<p>datanode网络流量监控：<br/>
以下是下线datanode以外的其他datanode在block transfer过程中的网络数据流量监控图：</p>

<p><img src="http://luoli523.github.com/static/dn-decom/net-monitor3.PNG" alt="net-monitor3" />
<img src="http://luoli523.github.com/static/dn-decom/net-monitor4.PNG" alt="net-monitor4" /></p>

<p>另外，从网卡流量监控脚本的监控数据来看，日志节选如下：</p>

<p><img src="http://luoli523.github.com/static/dn-decom/network5.PNG" alt="network5" /></p>

<p>从以上统计也可以非常明显的看出，在做block copy的这段时间内，bond0网卡的input KB/s流量为：<strong>39895.9</strong> kBytes/s，output KB/s 流量为：<strong>40049</strong> kBytes/s。这个值跟ganglia监控图上的流量统计基本符合。</p>

<h5>128MBlockSize balanceThrottler 20M/s</h5>

<ul>
<li>blocksize：128MB</li>
<li>balanceThrottler：20MB/s <br/>
下线测试集群中的dw1，包含blocks数为5757个，过程数据记录如下：<br/>
过程耗时：12分22秒（10:29:12 ～10:41:34）<br/>
统计该metasave日志，结果如下：</li>
<li>整个block transfer过程：742秒</li>
<li>拷贝blocks数：5757个 <br/>
以下是下线datanode以外的其他datanode在block transfer过程中的网络数据流量监控图：</li>
</ul>


<p><img src="http://luoli523.github.com/static/dn-decom/net-monitor4.PNG" alt="net-monitor5" /></p>

<p>另外，从网卡流量监控脚本的监控数据来看，日志节选如下：</p>

<p><img src="http://luoli523.github.com/static/dn-decom/network6.PNG" alt="network6" /></p>

<p>从以上统计也可以非常明显的看出，在做block copy的这段时间内，bond0网卡的input KB/s流量为：<strong>57197.3</strong> kBytes/s，output KB/s 流量为：<strong>56385.1</strong> kBytes/s。这个值跟ganglia监控图上的流量统计基本符合。</p>

<h3>总结</h3>

<p>根据blocksize的不同整理对比如下：<br/>
<img src="http://luoli523.github.com/static/dn-decom/summary.PNG" alt="summary" /></p>

<ol>
<li>dfs.balance.bandwidthPerSec（默认1MB/s）配置选项只能用来限制在pipeline的情况下或者做Balancer的情况下block copy所占用的datanode网络带宽，没有办法限制在datanode下线时造成的block copy产生的网络带宽</li>
<li>dfs.max-repl-streams（默认2）配置选项能够影响datanode下线时每个datanode同时能够进行多少个block的并行拷贝，调大该选项有利于加速datanode下线时的block copy。但是同时也会增加datanode下线对带宽的更高占用，进而有可能对应用程序产生影响。鉴于以上测试数据，建议非特殊情况不要调大。</li>
<li>BlockSize的大小在block copy过程中对带宽的占用是有影响的，block size越大，datanode花费在socket初始化上的时间越少，数据的传输在一定时间内对网络带宽的消耗会更加高一些。当block到128MB甚至更大时，block copy产生的带宽将非常可观。</li>
<li>集群的规模越大，有利于缓解在datanode下线产生的大量block copy的过程中带来的带宽占用的时间，因为datanode越多，分到每个datanode上进行block copy的命令就相应较少。</li>
<li>多台datanode下线跟一台datanode下线的情况的区别仅仅在于在namenode内部，要进行block copy的block变多了，本质上，这一过程只跟需要进行transfer的block相关，跟下线datanode的台数无关。</li>
<li>下线的datanode上保存的block越多，造成的影响时间会越久。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SecondaryNamenode做checkpoint时隐藏的一个性能瓶颈]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/13/secondarynamenodezuo-checkpointshi-yin-cang-de-%5B%3F%5D-ge-xing-neng-ping-jing/"/>
    <updated>2012-11-13T22:41:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/13/secondarynamenodezuo-checkpointshi-yin-cang-de-[?]-ge-xing-neng-ping-jing</id>
    <content type="html"><![CDATA[<p>这个问题很隐晦，很难发现，而且通常也不会造成什么大的问题，但是当集群规模非常大，势必fsimage也会逐渐膨胀，那么当SecondaryNameNode做checkpoint的时候，效率会比fix这个问题的版本要低，带来的网络消耗也会比fix这个问题的版本要大。</p>

<p>会触发这个性能隐患有几个条件：</p>

<ol>
<li>hadoop版本刚好有这个问题(0.20.2以及以前的版本/cdh3u0)</li>
<li>namenode的fsimage和edits log写入点配置多个，并且包括一个NFS写入点（比如dfs.name.dir=/path/on/local,/path/to/nfs）</li>
<li>NFS远程的mount点刚好在SNN的local磁盘上</li>
<li>fsimage非常的大（云梯已经接近40G了）</li>
</ol>


<p>如下图所示：</p>

<!-- more -->


<p><img src="http://luoli523.github.com/static/nn-snn-checkpoint-nfs/nn-snn-mount.PNG" alt="nn-snn-mount" /></p>

<p>如果集群再非常的繁忙，那么在很短的时间内，edits log也会被写的非常的大（云梯上不用一个小时就能写好几个G），那么这个问题就更加的凸显出来。</p>

<p>以下说一说这个问题的始末：<br/>
如下图所示，SNN在做checkpoint的时候，是如下一个流程：<br/>
<img src="http://luoli523.github.com/static/nn-snn-checkpoint-nfs/nn-snn-checkpoint.PNG" alt="nn-snn-checkpoint" /></p>

<p>简单说来，就如下几步：</p>

<ol>
<li>SNN调用NN的rollEditLog rpc接口，让NN将新的editlog写入到edits.new中</li>
<li>SNN通过http从NN上下载fsimage</li>
<li>SNN通过http从NN上下载edits</li>
<li>SNN将下载来的fsimage和edits进行内存加载，并将内存中新加载的namespace重新save成一个新的fsimage</li>
<li>SNN将新merge的fsimage通过http回传给NN</li>
<li>SNN调用NN的rollFsImage rpc接口，完成这次checkpoint过程</li>
</ol>


<p>在这里，出现性能不优的地方在第2和第3步，也就是SNN从NN上下载fsimage和edits文件的时候。原因在于：<br/>
如果满足以上所说的触发条件，那么当SNN从NN进行下载的时候，Namenode#FSImage中有这么一段代码：</p>

<figure class='code'><figcaption><span>FSImage#getFsImageName()  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Return the name of the image file.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="n">File</span> <span class="nf">getFsImageName</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>       <span class="n">StorageDirectory</span> <span class="n">sd</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>       <span class="k">for</span> <span class="o">(</span><span class="n">Iterator</span><span class="o">&lt;</span><span class="n">StorageDirectory</span><span class="o">&gt;</span> <span class="n">it</span> <span class="o">=</span>  <span class="n">dirIterator</span><span class="o">(</span><span class="n">NameNodeDirType</span><span class="o">.</span><span class="na">IMAGE</span><span class="o">);</span> <span class="n">it</span><span class="o">.</span><span class="na">hasNext</span><span class="o">();)</span>
</span><span class='line'>           <span class="n">sd</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="na">next</span><span class="o">();</span>
</span><span class='line'>       <span class="k">return</span> <span class="nf">getImageFile</span><span class="o">(</span><span class="n">sd</span><span class="o">,</span> <span class="n">NameNodeFile</span><span class="o">.</span><span class="na">IMAGE</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>这段代码中有一个问题，那就是，在返回fsimage文件的时候，是选择了最后一个写入点，而通常在配置dfs.name.dir这个选项的时候，很自然的会把本地磁盘写入点放在前面，将NFS的写入点放在后面（比如dfs.name.dir=/path/on/local,/path/to/nfs），这样，就会返回NFS的那个写入点给SNN进行读取和下载，造成的后果就是：SNN从NN的NFS写入点进行fsimage的读取，并下载到本地，而这个NFS的写入点本身是mount在SNN自己这台机器上的。所以，这样一个过程中，SNN是通过网络，从另外一台机器上访问自己机器上的目录，并再通过网络传回自己机器上。。。。。。</p>

<p>以下是修改这个性能瓶颈的代码之前，SNN在做Checkpoint时的网络流量监控图：<br/>
<img src="http://luoli523.github.com/static/nn-snn-checkpoint-nfs/snn-checkpoint-nfs-beforefix2.png" alt="snn-checkpoint-nfs-beforefix2" />
从上图就可以很明显的发现，在进行下载的阶段（14：35那个高峰），SNN上的网络流量是即有In又有Out，而照正常逻辑，应该是只有In，没有Out才对，这里出现这样的原因，就是因为这个NFS的回环流量导致。</p>

<p>要解决这个问题其实很简单，只要在刚才的代码里，在访问到第一个可用的写入点的时候就立刻返回，而不是遍历到最后一个即可。比如：</p>

<figure class='code'><figcaption><span>FSImage#getFsImageName()  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>    <span class="cm">/**</span>
</span><span class='line'><span class="cm">     * Return the name of the image file.</span>
</span><span class='line'><span class="cm">     */</span>
</span><span class='line'>    <span class="n">File</span> <span class="nf">getFsImageName</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">StorageDirectory</span> <span class="n">sd</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>      <span class="k">for</span> <span class="o">(</span><span class="n">Iterator</span><span class="o">&lt;</span><span class="n">StorageDirectory</span><span class="o">&gt;</span> <span class="n">it</span> <span class="o">=</span> <span class="n">dirIterator</span><span class="o">(</span><span class="n">NameNodeDirType</span><span class="o">.</span><span class="na">IMAGE</span><span class="o">);</span> <span class="n">it</span><span class="o">.</span><span class="na">hasNext</span><span class="o">();)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">sd</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="na">next</span><span class="o">();</span>
</span><span class='line'>        <span class="n">File</span> <span class="n">fsImage</span> <span class="o">=</span> <span class="n">getImageFile</span><span class="o">(</span><span class="n">sd</span><span class="o">,</span> <span class="n">NameNodeFile</span><span class="o">.</span><span class="na">IMAGE</span><span class="o">);</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">sd</span><span class="o">.</span><span class="na">getRoot</span><span class="o">().</span><span class="na">canRead</span><span class="o">()</span> <span class="o">&amp;&amp;</span> <span class="n">fsImage</span><span class="o">.</span><span class="na">exists</span><span class="o">())</span>
</span><span class='line'>          <span class="k">return</span> <span class="n">fsImage</span><span class="o">;</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">return</span> <span class="kc">null</span><span class="o">;</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>问题即可解决：<br/>
以下是问题解决后再次进行checkpoint时的流量图：<br/>
<img src="http://luoli523.github.com/static/nn-snn-checkpoint-nfs/snn-checkpoint-nfs-afterfix.png" alt="snn-checkpoint-nfs-afterfix" />
非常的明显，在下载阶段，SNN上只有In的流量，没有了Out的流量。问题解决<br/>
写在最后：其实类似这样的问题，要解决非常的简单，几行代码的修改就可以解决问题，但是，发现问题的过程需要非常好的监控系统和很细心的对线上系统能够各种运行情况的观察。更直接一点，需要的是对系统内部运行机制烂熟于胸的熟悉程度和对系统运行情况的密切关注，做到了这两点，就能发现更多这样的问题。</p>

<p>如果你的集群用的是0.20.2以及以前的版本或者cdh3u0,并且如上所述配置了NFS，请检查一下你的系统，没准也有这个问题：）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[11.11跟同事们并肩镇守云梯]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/10/11-dot-11gen-tong-shi-men-bing-jian-zhen-shou-yun-ti/"/>
    <updated>2012-11-10T22:21:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/10/11-dot-11gen-tong-shi-men-bing-jian-zhen-shou-yun-ti</id>
    <content type="html"><![CDATA[<p>11月09日，公司内部，不管是北京还是杭州，都弥漫着一股“山雨欲来风满楼”的氛围。所有的人所有的团队，都已经整装代发。核心系统部的同事早早的就开赴杭州了。我也独自上路，跟杭州云梯的兄弟们并肩作战</p>

<!-- more -->


<p>11月10日上午，公司楼下已经媒体云集，都是来报道双11活动的。媒体跟我们的指挥部之间被立起来的墙给隔开了。公司早早的给我们交待了：不能私自接受媒体采访，不能随意透露关键数据和关键决策，总之一句话：只干活，别说话-_-</p>

<p>11月10日下午四点指挥部开始集合，我们被领进了“小黑屋”里，里面有吃的，有喝得，甚至连床铺都有。看起来，所有人都准备大干一场。刚开始的时候大家还挺轻松，因为零点还没到，有些同事还有时间上tmall上去抢几个红包，随着时间逐渐逼近零点，紧张的气氛开始蔓延，大家紧盯着屏幕，等待即将到来的狂潮。</p>

<p>11月11日凌晨0:00一到，汹涌的流量如约而至！其实事先已经有思想准备，但还是没有想到会有这么大。几乎就是在一瞬间，汹涌的用户们开始了他们的血拼。也几乎在同一时间，指挥部旺旺群里开始实时传送线上的情况。10分钟后，支付宝交易额突破2.5亿；紧接着，一个一个的数字不断的冲击着我们预期的极限，不断的超越。已经没有办法去看这些交易数字，在我们眼里，各种iops，各种流量，各种稳定性，各种数据量，各种拥塞在不断的攀升，我们已经没有时间关注别的了。</p>

<p>凌晨3点，刚开始的仓促和拥塞逐渐被抢修过来，曲线开始在高位趋于平缓。数字在不断的刷新，我们仍在坚守岗位。我好歹还中途睡了几个小时，有几位兄弟整晚没睡。。。</p>

<p>现在时间11月11日下午8点半，今晚将迎来云梯的大考，身边的人都显露出了疲态，但我们还在坚守。兄弟们，Itis my honored to fight with you</p>

<p><img src="http://luoli523.github.com/static/1111/IMAG0352.jpg" alt="IMAG0352.jpg" />
<img src="http://luoli523.github.com/static/1111/IMAG0369.jpg" alt="IMAG0369.jpg" />
<img src="http://luoli523.github.com/static/1111/IMAG0380.jpg" alt="IMAG0380.jpg" />
<img src="http://luoli523.github.com/static/1111/IMAG0394.jpg" alt="IMAG0394.jpg" />
<img src="http://luoli523.github.com/static/1111/IMAG0395.jpg" alt="IMAG0395.jpg" />
<img src="http://luoli523.github.com/static/1111/IMAG0396.jpg" alt="IMAG0396.jpg" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop作业调优参数整理]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/08/hadoopzuo-ye-diao-you-can-shu-zheng-li/"/>
    <updated>2012-11-08T20:50:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/08/hadoopzuo-ye-diao-you-can-shu-zheng-li</id>
    <content type="html"><![CDATA[<p>新博客开张几天，有好多以前看过我CSDN上博客的同行们在问我以前的笔记还会不会整理到新博客上来，其实原本没有打算搞上来的，因为一来挺耗精力，二来也都比较老了，有一些内容可能大家都已经熟悉，或者有些已经过时了。实在没有想到居然还会有那么多关注以前文章。所以决定，把以前的一些笔记都还是整理到新博客上来。有的可能比较老了，大家请多包含。</p>

<p>今天整理的是这一篇：“hadoop作业调优参数整理”</p>

<!-- more  -->


<h2>Map side tuning参数</h2>

<h3>MapTask运行内部原理</h3>

<p><img src="http://luoli523.github.com/static/mapTask.jpg" alt="mapTaks" /></p>

<p>开始运算，并产生中间数据时，其产生的中间结果并非直接就简单的写入磁盘。这中间的过程比较复杂，并且利用到了内存buffer来进行已经产生的部分结果的缓存，并在内存buffer中进行一些预排序来优化整个map的性能。如上图所示，每一个map都会对应存在一个内存buffer（MapOutputBuffer，即上图的buffer in memory），map会将已经产生的部分结果先写入到该buffer中，这个buffer默认是100MB大小，但是这个大小是可以根据job提交时的参数设定来调整的，该参数即为：io.sort.mb。当map的产生数据非常大时，并且把io.sort.mb调大，那么map在整个计算过程中spill的次数就势必会降低，map task对磁盘的操作就会变少，如果map tasks的瓶颈在磁盘上，这样调整就会大大提高map的计算性能。map做sort和spill的内存结构如下如所示：<br/>
<img src="http://luoli523.github.com/static/mapSpill1.jpg" alt="mapSpill1" /></p>

<p>map在运行过程中，不停的向该buffer中写入已有的计算结果，但是该buffer并不一定能将全部的map输出缓存下来，当map输出超出一定阈值（比如100M），那么map就必须将该buffer中的数据写入到磁盘中去，这个过程在mapreduce中叫做spill。map并不是要等到将该buffer全部写满时才进行spill，因为如果全部写满了再去写spill，势必会造成map的计算部分等待buffer释放空间的情况。所以，map其实是当buffer被写满到一定程度（比如80%）时，就开始进行spill。这个阈值也是由一个job的配置参数来控制，即io.sort.spill.percent，默认为0.80或80%。这个参数同样也是影响spill频繁程度，进而影响map task运行周期对磁盘的读写频率的。但非特殊情况下，通常不需要人为的调整。调整io.sort.mb对用户来说更加方便。</p>

<p>当map task的计算部分全部完成后，如果map有输出，就会生成一个或者多个spill文件，这些文件就是map的输出结果。map在正常退出之前，需要将这些spill合并（merge）成一个，所以map在结束之前还有一个merge的过程。merge的过程中，有一个参数可以调整这个过程的行为，该参数为：io.sort.factor。该参数默认为10。它表示当merge spill文件时，最多能有多少并行的stream向merge文件中写入。比如如果map产生的数据非常的大，产生的spill文件大于10，而io.sort.factor使用的是默认的10，那么当map计算完成做merge时，就没有办法一次将所有的spill文件merge成一个，而是会分多次，每次最多10个stream。这也就是说，当map的中间结果非常大，调大io.sort.factor，有利于减少merge次数，进而减少map对磁盘的读写频率，有可能达到优化作业的目的。</p>

<p>当job指定了combiner的时候，我们都知道map介绍后会在map端根据combiner定义的函数将map结果进行合并。运行combiner函数的时机有可能会是merge完成之前，或者之后，这个时机可以由一个参数控制，即min.num.spill.for.combine（default 3），当job中设定了combiner，并且spill数最少有3个的时候，那么combiner函数就会在merge产生结果文件之前运行。通过这样的方式，就可以在spill非常多需要merge，并且很多数据需要做conbine的时候，减少写入到磁盘文件的数据数量，同样是为了减少对磁盘的读写频率，有可能达到优化作业的目的。</p>

<p>减少中间结果读写进出磁盘的方法不止这些，还有就是压缩。也就是说map的中间，无论是spill的时候，还是最后merge产生的结果文件，都是可以压缩的。压缩的好处在于，通过压缩减少写入读出磁盘的数据量。对中间结果非常大，磁盘速度成为map执行瓶颈的job，尤其有用。控制map中间结果是否使用压缩的参数为：mapred.compress.map.output(true/false)。将这个参数设置为true时，那么map在写中间结果时，就会将数据压缩后再写入磁盘，读结果时也会采用先解压后读取数据。这样做的后果就是：写入磁盘的中间结果数据量会变少，但是cpu会消耗一些用来压缩和解压。所以这种方式通常适合job中间结果非常大，瓶颈不在cpu，而是在磁盘的读写的情况。说的直白一些就是用cpu换IO。根据观察，通常大部分的作业cpu都不是瓶颈，除非运算逻辑异常复杂。所以对中间结果采用压缩通常来说是有收益的。以下是一个wordcount中间结果采用压缩和不采用压缩产生的map中间结果本地磁盘读写的数据量对比：</p>

<ul>
<li><p>map中间结果不压缩：<br/>
<img src="http://luoli523.github.com/static/job_un_codec.jpg" alt="job_un_codec" /></p></li>
<li><p>map中间结果压缩：<br/>
<img src="http://luoli523.github.com/static/job_codec.jpg" alt="job_codec" /></p></li>
</ul>


<p>可以看出，同样的job，同样的数据，在采用压缩的情况下，map中间结果能缩小将近10倍，如果map的瓶颈在磁盘，那么job的性能提升将会非常可观。</p>

<p>当采用map中间结果压缩的情况下，用户还可以选择压缩时采用哪种压缩格式进行压缩，现在hadoop支持的压缩格式有：GzipCodec，LzoCodec，BZip2Codec，LzmaCodec等压缩格式。通常来说，想要达到比较平衡的cpu和磁盘压缩比，LzoCodec比较适合。但也要取决于job的具体情况。用户若想要自行选择中间结果的压缩算法，可以设置配置参数：mapred.map.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec或者其他用户自行选择的压缩方式。</p>

<h3>Map side相关参数调优</h3>

<pre><code>选项                                      类型    默认值         描述
io.sort.mb                                int        100            缓存map中间结果的buffer大小(in MB)
io.sort.record.percent                    float    0.05         io.sort.mb中用来保存map output记录边界的百分比，其他缓存用来保存数据
io.sort.spill.percent                       float      0.80         map开始做spill操作的阈值
io.sort.factor                            int        10           做merge操作时同时操作的stream数上限。
min.num.spill.for.combine                   int      3            combiner函数运行的最小spill数
mapred.compress.map.output                boolean    false        map中间结果是否采用压缩
mapred.map.output.compression.codec       classname org.apache.hadoop.io.compress.DefaultCodec map中间结果的压缩格式
</code></pre>

<h2>Reduce side tuning参数</h2>

<h3>ReduceTask运行内部原理</h3>

<p><img src="http://luoli523.github.com/static/reduceTask.jpg" alt="reduceTask" />
reduce的运行是分成三个阶段的。分别为copy->sort->reduce。由于job的每一个map都会根据reduce(n)数将数据分成map 输出结果分成n个partition，所以map的中间结果中是有可能包含每一个reduce需要处理的部分数据的。所以，为了优化reduce的执行时间，hadoop中是等job的第一个map结束后，所有的reduce就开始尝试从完成的map中下载该reduce对应的partition部分数据。这个过程就是通常所说的shuffle，也就是copy过程。</p>

<p>Reduce task在做shuffle时，实际上就是从不同的已经完成的map上去下载属于自己这个reduce的部分数据，由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载，这个并行度是可以调整的，调整参数为：mapred.reduce.parallel.copies（default 5）。默认情况下，每个只会有5个并行的下载线程在从map下数据，如果一个时间段内job完成的map有100个或者更多，那么reduce也最多只能同时下载5个map的数据，所以这个参数比较适合map很多并且完成的比较快的job的情况下调大，有利于reduce更快的获取属于自己部分的数据。</p>

<p>reduce的每一个下载线程在下载某个map数据的时候，有可能因为那个map中间结果所在机器发生错误，或者中间结果的文件丢失，或者网络瞬断等等情况，这样reduce的下载就有可能失败，所以reduce的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从另外的地方下载（因为这段时间map可能重跑）。所以reduce下载线程的这个最大的下载时间段是可以调整的，调整参数为：mapred.reduce.copy.backoff（default 300秒）。如果集群环境的网络本身是瓶颈，那么用户可以通过调大这个参数来避免reduce下载线程被误判为失败的情况。不过在网络环境比较好的情况下，没有必要调整。通常来说专业的集群网络不应该有太大问题，所以这个参数需要调整的情况不多。</p>

<p>Reduce将map结果下载到本地时，同样也是需要进行merge的，所以io.sort.factor的配置选项同样会影响reduce进行merge时的行为，该参数的详细介绍上文已经提到，当发现reduce在shuffle阶段iowait非常的高的时候，就有可能通过调大这个参数来加大一次merge时的并发吞吐，优化reduce效率。</p>

<p>Reduce在shuffle阶段对下载来的map数据，并不是立刻就写入磁盘的，而是会先缓存在内存中，然后当使用内存达到一定量的时候才刷入磁盘。这个内存大小的控制就不像map一样可以通过io.sort.mb来设定了，而是通过另外一个参数来设置：mapred.job.shuffle.input.buffer.percent（default 0.7），这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。也就是说，如果该reduce task的最大heap使用量（通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</p>

<p>假设mapred.job.shuffle.input.buffer.percent为0.7，reduce task的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右，这700M的内存，跟map端一样，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷。这个限度阈值也是可以通过job参数来设定的，设定参数为：mapred.job.shuffle.merge.percent（default 0.66）。如果下载速度很快，很容易就把内存缓存撑大，那么调整一下这个参数有可能会对reduce的性能有所帮助。</p>

<p>当reduce将所有的map上对应自己partition的数据下载完成后，就会开始真正的reduce计算阶段（中间有个sort阶段通常时间非常短，几秒钟就完成了，因为整个下载阶段就已经是边下载边sort，然后边merge的）。当reduce task真正进入reduce函数的计算阶段的时候，有一个参数也是可以调整reduce的计算行为。也就是：mapred.job.reduce.input.buffer.percent（default 0.0）。由于reduce计算时肯定也是需要消耗内存的，而在读取reduce需要的数据时，同样是需要内存作为buffer，这个参数是控制，需要多少的内存百分比来作为reduce读已经sort好的数据的buffer百分比。默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读处理数据。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，反正reduce的内存闲着也是闲着。</p>

<h3>Reduce side相关参数调优</h3>

<pre><code>选项                                      类型    默认值         描述
mapred.reduce.parallel.copies               int       5         每个reduce并行下载map结果的最大线程数
mapred.reduce.copy.backoff                int        300          reduce下载线程最大等待时间（in sec）
io.sort.factor                            int         10          同上
mapred.job.shuffle.input.buffer.percent   float     0.7         用来缓存shuffle数据的reduce task heap百分比
mapred.job.shuffle.merge.percent            float       0.66        缓存的内存中多少百分比后开始做merge操作
mapred.job.reduce.input.buffer.percent    float     0.0       sort完成后reduce计算阶段用来缓存数据的百分比
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop rpc异步返回机制，大幅降低namenode processTime和queueTime]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-%2Cda-fu-jiang-di-namenode-processtimehe-queuetime/"/>
    <updated>2012-11-04T16:00:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/04/hadoop-rpcyi-bu-fan-hui-ji-zhi-,da-fu-jiang-di-namenode-processtimehe-queuetime</id>
    <content type="html"><![CDATA[<p>在我们的集群里，namenode是关注度最高的一个地方，尤其是作业运行开始变慢，集群吞吐开始下降的时候，虽然有可能是其他的原因，但第一个被想到的，总是namenode，俨然有一种superstar的感觉。 <br/>
其实这是正常的。虽然只有在集群高负荷运行的时候，namenode的吞吐才会直接影响到整个集群的效率，但是。。。。下面这张图是云梯集群全天24小时map和reduce计算槽位的运行情况: <br/>
<img src="http://luoli523.github.com/static/MR-running.GIF" alt="MR daily running" /></p>

<p>直接一句话就是：除了每天0点到9点的生产时段，云梯集群全天无间隙的满负载运行。所以几乎每天，namenode的吞吐都是集群管理员关注的焦点。毫不夸张的说，我对namenode各项性能指标的熟悉程度，甚至要超过我对我家厨房里碗和调羹的个数和我家冰箱里还剩下多少杯酸奶。</p>

<p>最近做了一个比较大的改动，从测试的数据来看，效果不错。所以把中间的细节整理了一下：</p>

<!-- more -->


<p>namenode服务的时候，其实他运行的方式非常的简单，对文件系统的操作无非就是以下这些： <br/>
1. create<br/>
2. mkdir <br/>
3. rename (mv)<br/>
4. delete<br/>
5. complete (close)<br/>
6. setPermission<br/>
7. getFileInfo<br/>
8. getListing (ls)<br/>
9. &#8230;..</p>

<p>不管是在在传统的文件系统里还是在分布式的文件系统里，对文件系统的操作无非也就是这些。只不过在分布式文件系统里，这些操作都是通过RPC的方式来调用的。所以所白了，namenode做的事情，就是通过RPC Server对来自Client的以上各种请求进行响应而已。所以，抛开内部细节，从一个很上层的角度去看，namenode工作原理就如下图所示（因为这里要讲述的重点不在NN内存结构内部，其中NN内部内存结构的很多细节忽略了）： <br/>
<img src="http://luoli523.github.com/static/nn-no-syncthread.PNG" alt="nn-no-syncthread" /></p>

<p>从上图就可以看出，影响namenode性能的几个点，忽略掉一些细节，从一个很高的层面来看，分别是一下几个地方： <br/>
* Rpc Server接收rpc调用的效率 <br/>
* NN内存结构内在加读锁或者写锁后的处理效率 <br/>
* editLog的sync效率 <br/>
* Rpc的返回效率</p>

<p>在我们的优化之前，namenode处理一个一个rpc的流程就如上图所示，流程如下:</p>

<ol>
<li>Client通过rpc向namenode rpc server发起一个请求（如mkdir）</li>
<li>namenode Rpc Server的listener线程accept这个请求</li>
<li>Listener的子线程Reader从client读取调用函数和参数,并将这些数据抽象成一个Call对象,存在CallQueue中,等待handler处理</li>
<li>Handler线程中某一个空闲的线程从CallQueue中取出一个Call(比如就是刚才的mkdir),然后发现是要加写锁的操作,于是等待NN的WriteLock</li>
<li>拿到WriteLock写锁以后,在NN内部数据结构中创建一个目录(dir)</li>
<li>将这个对namespace的修改记录到(sync)editlog中</li>
<li>释放NN写锁</li>
<li>将这个调用的返回交给Responder线程</li>
<li>Responder线程在获取CPU时间片后向Client返回这次调用(成功or失败)</li>
</ol>


<p>至此,一次rpc的调用过程结束.</p>

<p>从这个过程可以很明显的看出来, 整个环节中,任何一个地方都有可能成为瓶颈, 我们一轮一轮的优化就是在解决了最耗时的地方瓶颈后,瓶颈点转移到另外的地方,然后下一轮接着优化的这样一个过程.  <br/>
这次的修改主要针对的是以下的瓶颈点(以前几次优化针对的是其他的地方,所以我会在接下来的笔记里详细记录):</p>

<p>在第6步，当这种加写锁的操作一旦过多，那么由于每一次加写锁的操作实际上都修改了整个文件系统的namespace，所以为了数据一致性的保证，必须要将这样的一次修改记录在editlog中（相当于mysql中的redo log），而我们的editlog为了保证数据可靠，配置了两个写入点，一个是namenode本地磁盘（12块SAS盘+Raid10卡），一个是通过NFS写入到另外一台远程的机器。当对editlog的sync变多时，由于editlog是顺序写入，那么就导致很多要sync edit的调用都等在这个地方无法交给Responder线程进行返回，也就无法释放NN的写锁。进而拖慢整个namenode的吞吐和响应速度。通常在这种时候，云梯hadoop用户的旺旺群里就会有用户问：“云梯怎么这么慢？又挂了？”</p>

<p>针对这种情况，这次做的优化是这样的一种思路：</p>

<ul>
<li>由于在第6步sync editlog之前，其实整个调用已经可以返回给用户了，但是为了数据可靠性，就必须要等两个editlog的写入sync结束以后才能释放写锁，其他handler才能进一步处理其他的调用。所以，只要能够确保rpc在sync完editlog以后再返回这一点，那么，其实可以把等待sync完成的这段时间用来处理其他的rpc请求，就不会有数据丢失的风险。</li>
<li>也就是说，可以让rpc在完成第5步以后，等待sync完成之前，就将NN的写锁释放，并将sync的等待交给后台线程（SyncThread）去做，并在sync完成以后，将这个rpc的返回交给responder线程，那么就可以把所有rpc调用中等待sync磁盘的时间给释放出来，提供更多的服务。</li>
<li>修改过后，在第6步完成以后，其他的handler就不需要在等待writeLock，可以开始获取写锁开始服务</li>
</ul>


<p>因此，修改以后，整个过程变成如下形式： <br/>
<img src="http://luoli523.github.com/static/nn-syncthread.PNG" alt="nn-syncthread" /></p>

<p>从原理上分析，优化以后肯定会有如下效果：</p>

<ol>
<li>namenode响应中，rpc的processTime肯定会下降（因为没有了等待sync磁盘的时间）</li>
<li>rpc CallQueue队列中的queueTime(每个call在callqueue中的排队时间)肯定也会下降</li>
<li>namenode的整体吞吐肯定会上升</li>
<li>namenode的CPU利用率肯定会上升（等待时间变短）</li>
</ol>


<p>通过测试集群的压测（不得不说，分布式测试团队的同事们实在是给力啊），以下是详细的各项指标的数据变化情况：<br/>
<img src="http://luoli523.github.com/static/nn-syncthread-perf.PNG" alt="nn-syncthread-perf" /></p>

<p>不过这次优化因为一个bug上线回滚了，在线上运行的那段时间观察效果的确很明显，赶上双十一封网，跟团队同事商量后，为了保险起见，决定双十一以后再上线。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[跟facebook工程师交流HDFS笔记整理]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li/"/>
    <updated>2012-11-03T17:17:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/gen-facebookgong-cheng-shi-jiao-liu-hdfsbi-ji-zheng-li</id>
    <content type="html"><![CDATA[<p>hadoop在纽约的大会今年是10月22日～10月25日召开的。被公司派去美国参加hadoop大会，本来非常高兴，谁知道一切具备的情况下居然莫名其妙的被美国使馆给拒签了…… 连B1签证都被拒了，点儿实在是太背。没办法，本来事先约好的跟facebook hadoop团队的交流中我的那部分，只有由同事代劳了。听说FB的人得知我因为签证被拒而没有去，都很诡异的笑了 -_-</p>

<p>不过还好，虽然人没到场，但我要交流的topic和相应的细节都整理好了托同事带到了，然后通过事后的邮件来往，并不影响实质的交流。随着沟通的深入，我们的情况他们也都了解，他们的一些细节，我也都已经清楚。</p>

<p>很客观的讲，在开发方面（运维方面我们的ops团队实在是够专业：） <a href="http://weibo.com/u/1804480064">@淘大舞</a> <a href="http://weibo.com/u/1084192524">@dun_2010</a>），至少在存储这一部分，FB比我们做的好，走的比我们快。邮件的深入交流中，我整理了一下他们特别提到的几点。应该说，由于集群规模和数据规模很接近，使用的hadoop版本也接近，所以大家遇到的问题和解决的方式都基本是很接近的思路，这其实非常的神奇，他们自己都说：在另外的一个地方，有一个跟我们最大的集群差不多规模的集群，做着相同的事情，遇到相同的问题，真的是一件神奇的事情。</p>

<p>把交流的内容整理了一下，主要有一下几个方面：</p>

<!-- more -->


<h3>namenode heapsize</h3>

<p>这个问题我们和FB都遇到了，而且很客观的讲，全球所有的hadoop集群，碰到namenode heapSize触发java6 JDK bug的，可能就只有FB的warehouse集群和我们的云梯集群。这个bug导致的后果就是namenode的heapsize加大到130GB以后，再往上加就会crash，即使物理内存够（我们服务器的是192GB）也无法使用了。想要遇到这个问题，其实不是一件容易的事情，集群的规模没到，前期各种瓶颈没有解决，是不可能来到这个地方。不过对FB来说，他们的Federation已经在线上运行，所以遇到这个问题可以绕过去，而且他们已经在调研java7，估计在不久后会迁移上去（BTW，java7中这个问题的解决也是淘宝同事的贡献：））。对我们来说非常的不幸，由于federation还在开发阶段，就不得不直接面对。幸好我们的JVM组的同事给力（<a href="http://weibo.com/u/1920312980">@王王争</a> <a href="http://weibo.com/halmo">@坤谷</a>），拿到我们的反馈后，迅速做出响应，并将这个bug的解决patch提交给了java社区（详情请见 <a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7197906">bug详情</a> 和 <a href="http://cr.openjdk.java.net/~brutisso/7197906/webrev.02/">patch</a> BTW：我永远不会承认java跟Oracle这家律师事务所有任何的关系，SUN才是Java的娘家），问题解决。<br/>
<strong>如果你走在雷区的最前沿，趟雷的是你，为别人插旗指路的也是你。</strong></p>

<h3>储存优化</h3>

<p>由于namenode heapsize的问题，引发了很多的讨论，各自也都想过很多的办法。从开发上，FB开发了一套HDFS Raid系统，能够在不降低数据可靠性的情况下，利用Reed Solomon编码（RS编码），减少分布式文件对物理存储20%～30%的消耗。从Raid系统核心开发人员的沟通中了解到，Raid系统已经帮助FB节省了10.74 PB的物理存储空间。可能有人对这个没有太深的概念，但是，十几个PB的存储空间，几百万的资产。。。。 <br/>
经过几个月对代码的熟悉和修改，我们的Raid系统也与今年7月份成功上线，目前节省存储空间100T左右。我们的Raid系统才刚刚起步，刚上线不久，公司很多业务线都还不了解，还没有把自己的数据给Raid化，不过相信我们不遗余力的推广，成效会慢慢显现出来的。（这里其实不得不说，有时候在一些机构推行一个对大家都有利的东西，或者一个机制，真的好难。就好比给绝大部分的人两个选择：1，伴随着阵痛的治疗；2，无痛的死去，面对这样的选择，大部分的人居然会选择后者……原因很多，但有一点不可否认：绩效，让很多人失去闯劲和改变的勇气。治疗？改变？可没有KPI里的业绩重要。。。。不得不感谢量子团队和数据平台团队勇敢的成为第一个吃螃蟹的人，积极的配合我们去优化存储，造福大家。）越说越远了。。。赶紧拉回来！</p>

<h3>设计上的修改</h3>

<p>FB的同学还透露给我目前还在思考的一个从开发上的改进方案：<strong>可变blocksize方案。</strong> 目前在HDFS中，对一个文件进行切分的时候，都是按照固定blocksize进行切分，除了最后一个block，其他的block的size都是一样大。而如果实现了可变blocksize方案，这个格局将被打破，文件的不同block，size可以不一样（这一点其实不难做到，因为在namenode里，每个block对象的size其实都是一个独立的变量）。这样的后果就是：当要合并两个文件的时候，不再需要像现在一样将两个文件进行读出，然后顺序的写到另外一个merge的文件中去。也就是说，要合并文件，只需要一个调用，修改一点点meta信息，不产生任何的IO读写就能够做到。这将是一个不错的选择，不错的方案，能够以最小的代价合并HDFS上的小文件，减少meta信息对namenode内存的消耗。concatenate files，这将是我们团队接下来的一个很重要的工作。我会把细节和原理在将来的笔记里记录到这里来。</p>

<h3>RPC方面</h3>

<p>FB的做法跟我们一样，因为他们也发现了在namenode的个总rpc响应中，每一个rpc的处理时间里有很大的一部分是用来处理文件路径（breaking into components, getting bytes out of them），所以对namenode优化很大的一部分就是将这种类型的操作尽量的挪出读写锁。另外，FB的同学提到了一个新的想法：将namenode的rpc中，处理datanode的rpc和来自Client的rpc分开，用100个handler来处理datanode的请求，用更多的handler来处理Client的请求，根据他们的介绍，这能够对提高namendoe吞吐有很大的帮助。这是我们还没有尝试过的做法，肯定从程序的实现上做了比较大的修改，还有很多细节需要去了解才行。</p>

<h3>edits log</h3>

<p>目前FB也是使用了本地和NFS各一份的方式，不过也在做Bookkeeper和QJM的调研，呵呵，真的很神奇，大家的想法再一次不谋而合。</p>

<h3>Federation</h3>

<p>FB已经上线了他们的federation，实现跟社区稍有不一样，但原理基本一致。而且他们也是一个namenode一个pool的做法，这样简单。<strong>设计上完美的东西，永远都只能停留在paper和PPT里面，真正解决问题的，通常都是那些简单可靠的办法。</strong>（非常遗憾，在美国人看来，这样的做法简单高效，是最优方案，在很多中国人看来。。。。。 还不够吹毛求疵的。唉，又扯远了）</p>

<h3>Failover</h3>

<p>还有一点也非常神奇，FB的failover方案，目前来说也是人工介入的切换，并没有要做成自动切换。说白了，failover要解决的问题，99%都是升级不停服务的问题，为了TNND那1%的可能性去做那80%的工作，真的没有必要.</p>

<p>整理了一下，重要的就以上这些。真的是对我们接下来的工作有很多方向上的帮助。不得不说，跟FB的开发团队的交流有一种如沐春风的感觉。很单纯的一群人，听说他们的Raid系统在我们的集群也成功上线，他们都高兴的欢呼，击掌相庆。什么叫价值观？这才是价值观:)</p>

<p>自从删除CSDN上的博客以后，好久没有整理笔记了，以后我会把我们开发中的积累和经验都记下来，整理到这里来。呵呵，好记性，还是不如烂笔头的～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[新博客，第一篇，写给云梯]]></title>
    <link href="http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-%2Cdi-%5B%3F%5D-pian-%2Cxie-gei-yun-ti/"/>
    <updated>2012-11-03T00:19:00+08:00</updated>
    <id>http://luoli523.github.com/blog/2012/11/03/xin-bo-ke-,di-[?]-pian-,xie-gei-yun-ti</id>
    <content type="html"><![CDATA[<p>“数大就是美”，这是徐志摩同学《志摩日记二则》中的一句话：“数大”便是美。比如说：从无极的蓝空中下窥大地，是美；泰山顶上的云海，高大的云峰在晨光里镇定着，是美……。其实不仅仅这些，对于我来说，从云梯集群300台机器涨到现在3000多台，是美；云梯namenode的rpc吞吐一次次的上涨，是美；云梯的每一次性能数据的提升，也是美。</p>

<p>虽然过程并不是非常顺利，虽然伴随着各种惊心动魄和提心吊胆，虽然最新的一次升级经历了两次失败，但是昨天（2012-11-01），我们的云梯集群第一次超过了3000规模（3160），存储容量达到65PB，namenode平均每天处理rpc数量20+亿，集群DFS中文件和目录上2.2亿，blocks数2.8亿。到这种程度，我们所做的任何事，早已经超越了一般的程序设计和技术开发，每一次的升级，每一行代码的修改，都牵动着整个公司几千工程师和他们的工作，他们的业绩，和他们对我们的信心。每动一下手指，都是责任。对我一个小小的工程师来说，这样一份天大的责任，扛的实在是诚恐诚惶。最近的两次升级都失败了，引来了很多的抱怨和质疑，很对不起那些翘首企盼的同事们。不知道该说什么好，我能做的：尽人事，听天命。</p>

<p>新博客开张，第一篇，写给云梯。加油吧</p>

<p>BTW：感谢华仔帮我申请的域名，我想我会一直保留这个域名的。</p>
]]></content>
  </entry>
  
</feed>
